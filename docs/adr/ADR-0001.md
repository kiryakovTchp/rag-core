# ADR-0001: Minimal RAG Core Stack

Status: Accepted

Context
- We need a minimal, production-minded RAG core: ingestion of docs, chunking, embeddings storage in PostgreSQL (pgvector), and retrieval via vector search, optionally reranked by a cross-encoder.
- Requirements specify FastAPI, SQLAlchemy, Alembic, Docker Compose, and limited LangChain components: RecursiveCharacterTextSplitter and PGVector.

Decision
- API: FastAPI with Uvicorn. Clear separation between app (routes) and core (logic).
- DB: PostgreSQL 16 with pgvector (official pgvector image). SQLAlchemy 2.x ORM for documents/chunks. Alembic migrations enable the vector extension and create tables.
- Vector store: LangChain PGVector bound to the same Postgres. Embeddings stored via LangChain API to avoid bespoke schema.
- Embeddings: sentence-transformers, default BAAI/bge-small-en-v1.5 with normalized embeddings. Optional reranker BAAI/bge-reranker-v2-m3 via CrossEncoder.
- Parsing: PyMuPDF4LLM for PDFs; TXT is passed as-is; DOCX left as a stub.
- Chunking: RecursiveCharacterTextSplitter with defaults (size 500 chars, overlap 75) as a pragmatic proxy for 350–700 tokens.
- Observability: JSON logs with request_id via middleware.
- Dev: Dockerfile + docker-compose for local dev; migrations applied on container start. Pre-commit with ruff/black/mypy. pytest for tests; integration tests marked and require Postgres and models.

Consequences
- Embedding models download on first use, which may be slow; cached in container layers only if pre-fetched.
- We lean on LangChain PGVector’s tables alongside our own ORM tables. This avoids duplicating vector schema.
- Chunk sizes are character-based; for token-precise control, swap splitter or add tokenizer later.
- Health check verifies DB, pgvector extension, and ability to initialize embeddings provider.

