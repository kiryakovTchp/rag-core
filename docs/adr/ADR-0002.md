# ADR-0002: Indexing, DOCX parsing, token-based chunking, and API filters

Status: Accepted

Context
- Improve idempotency and query performance with proper DB indexes.
- Add DOCX ingestion using python-docx.
- Make chunking stable with token-accurate boundaries (not whitespace-sensitive).
- Add pagination and metadata filters to API.
- Enforce upload size limits and timeouts for robustness.

Decision
- DB: Alembic migration 0002 adds a partial unique index on `documents (meta->>'sha256')` to deduplicate uploads; reinforces indexes on `created_at` and `chunks(document_id)`.
- Parsing: Implement DOCX parser using python-docx; treat the entire DOCX as a single logical page (no pagination in format).
- Chunking: Switch to `RecursiveCharacterTextSplitter.from_tiktoken_encoder` with defaults `CHUNK_SIZE=600`, `CHUNK_OVERLAP=100` for token-based consistency.
- API: `/documents` supports `limit/offset/q`; `/query` supports metadata filters (`doc_id`, `source`, `section`) passed to PGVector `filter`.
- Security: Enforce `MAX_UPLOAD_MB=25` (413) and stricter `content-type` (415). Add rerank timeout with fallback on vector scores.
- CI: Add GitHub Actions workflow to run linters, mypy, alembic upgrade, and unit tests.

Consequences
- Unique index guarantees idempotent document records by file hash; ingestion first queries by hash and returns existing `doc_id`.
- DOCX ingestion enabled with minimal dependencies; complex structures (tables, images) are ignored for now.
- Token-based chunking requires `tiktoken` dependency; improves stability across whitespace.
- Vector retrieval can be narrowed by metadata filter for better precision.
- CI excludes integration tests to avoid model downloads; they remain available locally.

